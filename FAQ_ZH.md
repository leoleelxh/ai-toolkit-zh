# 常见问题解答

持续完善中。将根据需要继续添加内容。

## FLUX.1 训练

#### 训练 FLUX.1 的 LoRA 需要多少显存？

最少需要 24GB 显存。

#### 什么是 LoRA？

LoRA（Low-Rank Adaptation，低秩适应）是一种高效的模型微调技术。它不修改原始模型的所有参数，而是添加小的适配器模块。这样可以用很少的额外参数实现模型定制，同时保持较小的文件大小。

#### 什么是量化（Quantization）？

量化是一种减少模型内存使用的技术。它将模型权重从32位浮点数转换为更低精度的格式（如8位整数），从而显著减少显存占用，但可能会轻微影响模型精度。

#### 什么是梯度检查点（Gradient Checkpointing）？

梯度检查点是一种内存优化技术。它通过在反向传播时重新计算中间结果来节省显存，代价是增加一些计算时间。对于大模型在有限显存下训练非常重要。

#### 什么是混合精度训练？

混合精度训练使用不同精度的数值类型来平衡性能和精度。例如使用 bf16（bfloat16）可以减少显存使用并加速训练，同时保持足够的数值精度。

#### 如何选择学习率？

学习率控制模型权重更新的步长：
- 太高：训练不稳定，可能发散
- 太低：训练速度慢，可能陷入局部最优
- 建议从 1e-4 开始尝试，根据训练效果调整

#### batch_size 应该设置多少？

batch_size 受显存限制：
- 24GB 显存：通常设置为 1
- 更大显存：可以设置为 2-4
- 可以通过 gradient_accumulation_steps 模拟更大的 batch_size

#### 数据集应该准备多少图像？

推荐图像数量：
- 最少：4-8 张图像
- 理想：15-30 张图像  
- 最多：不超过 150 张图像
- 质量比数量更重要

#### 训练多少步合适？

步数建议：
- 少量数据（<10张）：500-1000 步
- 中等数据（10-30张）：1000-2000 步
- 较多数据（>30张）：2000-4000 步
- 观察过拟合现象及时停止